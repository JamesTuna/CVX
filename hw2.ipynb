{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Lasso </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lasso\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sub-gradient method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subGrad(iter,lr,regularization):\n",
    "    x = np.random.normal(0,0.1,A.shape[1])\n",
    "    iter = int(iter)\n",
    "    loss1 = []\n",
    "    loss2 = []\n",
    "    for i in range(iter):\n",
    "        # compute a subgradient\n",
    "        g = np.zeros(x.shape[0])\n",
    "        g[x>0] = 1\n",
    "        g[x<0] = -1\n",
    "        g *= regularization\n",
    "        g += ATA.dot(x) - ATb\n",
    "        x -=  lr * g\n",
    "        # calculate loss\n",
    "        loss_train = np.linalg.norm(A.dot(x)-b,2)\n",
    "        loss_test = np.linalg.norm(A_test.dot(x)-b_test,2)\n",
    "        loss1.append(loss_train)\n",
    "        loss2.append(loss_test)\n",
    "    return range(iter),loss1,loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $P_l$ operator in Breck's FISTA paper\n",
    "Gradient step followed by soft thresholding shirnkage towards 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Pl(x,lr,regularization):\n",
    "    # Pl() operator in paper Breck's FISTA paper\n",
    "    # compute a subgradient of g(x)\n",
    "    g = ATA.dot(x) - ATb\n",
    "    x -= lr *g\n",
    "    # soft threshold\n",
    "    eta = lr*regularization\n",
    "    x[np.abs(x)<=eta] = 0\n",
    "    x[x>eta] = x[x>eta] - eta\n",
    "    x[x<-eta] = x[x<-eta] + eta\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ISTA(iter,lr,regularization):\n",
    "    x = np.random.normal(0,0.1,A.shape[1])\n",
    "    iter = int(iter)\n",
    "    loss1 = []\n",
    "    loss2 = []\n",
    "    lipschitz = np.linalg.svd(ATA)[1][0]\n",
    "    for i in range(iter):\n",
    "        x = Pl(x,lr,regularization)\n",
    "        # calculate loss\n",
    "        loss_train = np.linalg.norm(A.dot(x)-b,2)\n",
    "        loss_test = np.linalg.norm(A_test.dot(x)-b_test,2)\n",
    "        loss1.append(loss_train)\n",
    "        loss2.append(loss_test)\n",
    "    return range(iter),loss1,loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FISTA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FISTA(iter,lr,regularization):\n",
    "    old_x = np.random.normal(0,0.1,A.shape[1])\n",
    "    old_y = old_x.copy()\n",
    "    old_t = 1\n",
    "    iter = int(iter)\n",
    "    loss1 = []\n",
    "    loss2 = []\n",
    "    lipschitz = np.linalg.svd(ATA)[1][0]\n",
    "    for i in range(iter):\n",
    "        new_x = Pl(old_y,lr,regularization)\n",
    "        new_t = (1+np.sqrt(1+4*old_t**2))/2\n",
    "        new_y = new_x + (new_x - old_x)*(old_t-1)/(new_t)\n",
    "        # calculate loss\n",
    "        x = new_y\n",
    "        loss_train = np.linalg.norm(A.dot(x)-b,2)\n",
    "        loss_test = np.linalg.norm(A_test.dot(x)-b_test,2)\n",
    "        loss1.append(loss_train)\n",
    "        loss2.append(loss_test)\n",
    "        # update x,y,t\n",
    "        old_x,old_y,old_t = new_x,new_y,new_t\n",
    "    return range(iter),loss1,loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frank Wolfe on radius-1000 ball"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FrankWolfe(iter,lr,regularization):\n",
    "    k = 1000\n",
    "    x = np.random.normal(0,0.1,A.shape[1])\n",
    "    iter = int(iter)\n",
    "    loss1 = []\n",
    "    loss2 = []\n",
    "    for i in range(iter):\n",
    "        # compute a subgradient\n",
    "        g = np.zeros(x.shape[0])\n",
    "        g[x>0] = 1\n",
    "        g[x<0] = -1\n",
    "        g *= regularization\n",
    "        g += ATA.dot(x) - ATb\n",
    "        # minimize < g , y > over radius-k ball\n",
    "        y = - k * g / np.linalg.norm(g)\n",
    "        x = x * (1-lr) + y * lr\n",
    "        # calculate loss\n",
    "        loss_train = np.linalg.norm(A.dot(x)-b,2)\n",
    "        loss_test = np.linalg.norm(A_test.dot(x)-b_test,2)\n",
    "        loss1.append(loss_train)\n",
    "        loss2.append(loss_test)\n",
    "    return range(iter),loss1,loss2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "All *.npy* files should be placed under \"data\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "A = np.load('./data/A_train.npy')\n",
    "b = np.load('./data/b_train.npy')\n",
    "\n",
    "A_test = np.load('./data/A_test.npy')\n",
    "b_test = np.load('./data/b_test.npy')\n",
    "\n",
    "ATA = A.T.dot(A)\n",
    "ATb = A.T.dot(b)\n",
    "beta = np.linalg.svd(ATA)[1][0] # smoothness of g()\n",
    "print('%.4f-smooth'%(beta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run 4 algorithms and draw plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up\n",
    "iter = 1e+4\n",
    "fig, axs = plt.subplots(1,2,sharex=True,figsize=(16,8))\n",
    "\n",
    "axs[1].set_title('Test loss')\n",
    "axs[0].set_title('Training loss')\n",
    "axs[1].set_xlabel('iteration')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[0].set_xlabel('iteration')\n",
    "axs[0].set_ylabel('loss')\n",
    "\n",
    "# subgradient method\n",
    "i,loss_train,loss_test = subGrad(iter,1e-6,20)\n",
    "l1, = axs[0].plot(i,loss_train,'blue',label='SGD')\n",
    "l5,=axs[1].plot(i,loss_test,'blue')\n",
    "\n",
    "beta *= 1000\n",
    "# ISTA\n",
    "i,loss_train,loss_test = ISTA(iter,1e-6,20)\n",
    "l2, = axs[0].plot(i,loss_train,'red',label='ISTA')\n",
    "l6,=axs[1].plot(i,loss_test,'red')\n",
    "\n",
    "# FISTA\n",
    "i,loss_train,loss_test = FISTA(iter,1e-6,20)\n",
    "l3, = axs[0].plot(i,loss_train,'black',label='FISTA')\n",
    "l7,=axs[1].plot(i,loss_test,'black')\n",
    "\n",
    "# FrankWolfe\n",
    "i,loss_train,loss_test = FrankWolfe(iter,1e-6,20)\n",
    "l4, = axs[0].plot(i,loss_train,'green',label='FW')\n",
    "l8,=axs[1].plot(i,loss_test,'green')\n",
    "\n",
    "axs[0].legend([l1,l2,l3,l4],['SGD','ISTA','FISTA','FW'],loc='best')\n",
    "axs[1].legend([l5,l6,l7,l8],['SGD','ISTA','FISTA','FW'],loc='best')\n",
    "\n",
    "# show plots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red>Logistic Regression</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "\n",
    "class LogisticRegression(object):\n",
    "\n",
    "    def __init__(self,n_feature,n_class,regularization = 0):\n",
    "        self.n_feature = n_feature\n",
    "        self.n_class = n_class\n",
    "        self.regularization = regularization\n",
    "\n",
    "    def fit(self,X,y,optimizer='GD',iter=1e+4,lr=1e-3,batch=32,log_path='./',log_interval=2000):\n",
    "        print(\"start training model...\")\n",
    "        print(\"%s iterations in total\"%iter)\n",
    "        if batch is not None:\n",
    "            print(\"stochastic %s with batch size %s\"%(optimizer,batch))\n",
    "        else:\n",
    "            print(\"%s: using all training data to compute gradient\"%optimizer)\n",
    "        \n",
    "        assert (X.shape[1]==self.n_feature), 'mismatch: number of features'\n",
    "        assert (y.max()<=self.n_class-1), 'mismatch: max index of classes'\n",
    "        assert (y.min()>=0), 'mismatch: min index of classes'\n",
    "\n",
    "        self.N = X.shape[0]\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.y_onehot = np.eye(self.n_class)[self.y]\n",
    "        self.B = np.random.normal(0,0.1,(self.n_feature,self.n_class))\n",
    "\n",
    "        test_loss_list = []\n",
    "        train_loss_list = []\n",
    "        test_acc_list = []\n",
    "        train_acc_list = []\n",
    "\n",
    "        # parameters for nesterov\n",
    "        AGD_lambda_old = 0\n",
    "        AGD_lambda_new = None\n",
    "        AGD_gamma = None\n",
    "        AGD_y_old = self.B\n",
    "        AGD_y_new = None\n",
    "\n",
    "        # log data\n",
    "        min_loss_test = np.inf\n",
    "        min_loss_iter = 0\n",
    "        min_loss_acc = 0\n",
    "        file_old = None\n",
    "\n",
    "        for i in range(iter):\n",
    "            \n",
    "\n",
    "            if optimizer == 'GD':\n",
    "                self.back_prop(batch=batch)\n",
    "                self.B += lr * (-self.G)\n",
    "                \n",
    "            \n",
    "            elif optimizer == 'AGD':\n",
    "                self.back_prop(batch=batch)\n",
    "                AGD_y_new = self.B - lr * self.G\n",
    "                AGD_lambda_new = (1+np.sqrt(1+4*AGD_lambda_old**2))/2\n",
    "                AGD_gamma = (1-AGD_lambda_old)/AGD_lambda_new\n",
    "                self.B = (1-AGD_gamma) * AGD_y_new + AGD_gamma * AGD_y_old\n",
    "\n",
    "                AGD_y_old = AGD_y_new\n",
    "                AGD_lambda_old = AGD_lambda_new\n",
    "\n",
    "            # test and record\n",
    "            if i % log_interval == 0:\n",
    "                loss,acc = self.getLoss(self.X,self.y,self.B,0)\n",
    "                test_loss,test_acc = self.getLoss(self.X_test,self.y_test,self.B,0)\n",
    "\n",
    "                test_loss_list.append(test_loss)\n",
    "                train_loss_list.append(loss)\n",
    "                test_acc_list.append(test_acc)\n",
    "                train_acc_list.append(acc)\n",
    "\n",
    "                if test_loss < min_loss_test:\n",
    "                    min_loss_test = test_loss\n",
    "                    min_loss_iter = i\n",
    "                    min_loss_acc = test_acc\n",
    "                print('iteration %s train loss %.4f test loss %.4f'%(i,loss,test_loss))\n",
    "\n",
    "        return train_loss_list,train_acc_list,test_loss_list,test_acc_list\n",
    "\n",
    "\n",
    "\n",
    "    def getLoss(self,X,y,B,regularization = 0):\n",
    "        N = X.shape[0]\n",
    "        scores = - X.dot(B) # N x n_class\n",
    "        prob_matrix = np.zeros((N,self.n_class))\n",
    "        for i in range(N):\n",
    "            for j in range(self.n_class):\n",
    "                prob_i_j = np.exp(scores[i,j])/np.exp(scores[i,:]).sum()\n",
    "                prob_matrix[i,j] = prob_i_j\n",
    "        loss = 0\n",
    "        for i in range(N):\n",
    "            loss += -scores[i,y[i]]\n",
    "            loss += np.log(np.exp(scores[i,:]).sum())\n",
    "        loss /= N\n",
    "        loss += regularization * ((B ** 2).sum())\n",
    "        predict = prob_matrix.argmax(axis=1)\n",
    "        accuracy = (predict == y[:N]).sum()/N\n",
    "        return loss,accuracy\n",
    "\n",
    "    def back_prop(self,batch=32):        # compute gradient, stored in self.G\n",
    "        if batch is None:\n",
    "            # using all samples to calculate gradient\n",
    "            Xs = self.X\n",
    "            onehot_ys = self.y_onehot\n",
    "            batch = self.X.shape[0]\n",
    "        else:\n",
    "            batch_samples = np.random.choice(self.N,batch,replace=False)\n",
    "            Xs = self.X[batch_samples]\n",
    "            onehot_ys = self.y_onehot[batch_samples]\n",
    " \n",
    "        class_scores = - Xs.dot(self.B) # N x n_class\n",
    "        probs = softmax(class_scores,axis=1)\n",
    "        self.G = Xs.T.dot(onehot_ys - probs)\n",
    "        # approximate expectation\n",
    "        self.G /= batch\n",
    "        # add graient of regularization term\n",
    "        self.G += 2 * self.regularization * self.G\n",
    "\n",
    "    def predict(self,X_test):\n",
    "        print(\"not implemented\")\n",
    "\n",
    "\n",
    "\n",
    "BATCH_SIZE = None # None means calculating gradient using all samples, change it to other values like 128, 256 and it becomes SGD\n",
    "LR = 0.1\n",
    "MU = 0\n",
    "ITER = 30000\n",
    "INTERVAL = 100 # how many iterations to record loss\n",
    "OPTIMIZER = 'AGD' # GD or AGD\n",
    "\n",
    "# load data\n",
    "X = np.array(pd.read_csv('data/logistic_news/X_train.csv',header=None))\n",
    "y = np.array(pd.read_csv('data/logistic_news/y_train.csv',header=None))[0]\n",
    "X_test = np.array(pd.read_csv('data/logistic_news/X_test.csv',header=None))\n",
    "y_test = np.array(pd.read_csv('data/logistic_news/y_test.csv',header=None))[0]\n",
    "print('number of training samples:',X.shape[0])\n",
    "\n",
    "# run SGD\n",
    "model = LogisticRegression(n_feature=X.shape[1],n_class=20,regularization = MU)\n",
    "model.X_test = X_test\n",
    "model.y_test = y_test\n",
    "train_loss,train_acc,test_loss,test_acc = model.fit(X,y,optimizer=OPTIMIZER,iter=ITER,lr=LR,batch=BATCH_SIZE,log_interval=INTERVAL)\n",
    "print('train loss %.4f test loss %.4f'%(train_loss[-1],test_loss[-1]))\n",
    "iterations = np.array([i for i in range(0,ITER*2,INTERVAL)])\n",
    "iterations = iterations[:len(train_acc)]\n",
    "print(len(train_acc),len(train_loss))\n",
    "# draw plots\n",
    "fig,axs = plt.subplots(1,2,figsize=(16,8))\n",
    "l1, = axs[0].plot(iterations,train_loss,'blue')\n",
    "l3, = axs[0].plot(iterations,test_loss,'red')\n",
    "axs[0].set_title('loss')\n",
    "axs[0].set_xlabel('iteration')\n",
    "axs[0].set_ylabel('loss')\n",
    "\n",
    "\n",
    "l2, = axs[1].plot(iterations,train_acc,'blue')\n",
    "l4, = axs[1].plot(iterations,test_acc,'red')\n",
    "axs[1].set_title('accuracy')\n",
    "axs[1].set_xlabel('iteration')\n",
    "axs[1].set_ylabel('accuracy')\n",
    "\n",
    "axs[0].legend([l1,l3],['train','test'],loc='best')\n",
    "axs[1].legend([l2,l4],['train','test'],loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Robust Regression and Mirror Descent </font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Robust Regression and Mirror Descent\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "\n",
    "class RobustRegression(object):\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self,X,y,iter=1000,lr=0.1):\n",
    "        train_loss_list = []\n",
    "        assert X.shape[0] == y.shape[0], 'mismatch number of samples and labels'\n",
    "        self.N, self.n_feature = X.shape\n",
    "        self.B = np.random.normal(0,1,self.n_feature)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(iter):\n",
    "            # gradient step\n",
    "            self.back_prop()\n",
    "            self.B += -lr*self.G\n",
    "            self.B = self.projsplx(self.B)\n",
    "            if i % 10 == 1:\n",
    "                loss = self.getLoss()\n",
    "                print('iter: %s, loss: %s'%(i,loss))\n",
    "                train_loss_list.append(loss)\n",
    "        return train_loss_list\n",
    "\n",
    "    def fit_MD(self,X,y,iter=1000,lr=0.1):\n",
    "        train_loss_list = []\n",
    "        assert X.shape[0] == y.shape[0], 'mismatch number of samples and labels'\n",
    "        self.N, self.n_feature = X.shape\n",
    "        self.B = np.random.normal(0,1,self.n_feature)\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        for i in range(iter):\n",
    "            self.back_prop()\n",
    "            # Mirror Descent step\n",
    "            # by solving min lr < gt,x > + D(x,xt)\n",
    "            # optimal point is xt/exp(lr gt)\n",
    "            self.B = self.B / np.exp(lr * self.G)\n",
    "            self.B = self.projsplx(self.B)\n",
    "\n",
    "            if i % 10 == 1:\n",
    "                loss = self.getLoss()\n",
    "                print('iter: %s, loss: %s'%(i,loss))\n",
    "                #print(self.B)\n",
    "                train_loss_list.append(loss)\n",
    "        return train_loss_list\n",
    "\n",
    "    def getLoss(self):\n",
    "        return np.linalg.norm(self.X.dot(self.B)-self.y)\n",
    "\n",
    "    def back_prop(self):\n",
    "        predict = self.X.dot(self.B) - self.y\n",
    "        predict[predict>0] = 1\n",
    "        predict[predict<0] = -1\n",
    "        self.G = self.X.T.dot(predict)\n",
    "\n",
    "    def projsplx(self,y):\n",
    "        \"\"\"projsplx projects a vector to a simplex\n",
    "        by the algorithm presented in\n",
    "        (Chen an Ye, \"Projection Onto A Simplex\", 2011)\"\"\"\n",
    "        assert len(y.shape) == 1\n",
    "        N = y.shape[0]\n",
    "        y_flipsort = np.flipud(np.sort(y))\n",
    "        cumsum = np.cumsum(y_flipsort)\n",
    "        t = (cumsum - 1) / np.arange(1,N+1).astype('float')\n",
    "        t_iter = t[:-1]\n",
    "        t_last = t[-1]\n",
    "        y_iter = y_flipsort[1:]\n",
    "        if np.all((t_iter - y_iter) < 0):\n",
    "            t_hat = t_last\n",
    "        else:\n",
    "        # find i such that t>=y\n",
    "            eq_idx = np.searchsorted(t_iter - y_iter, 0, side='left')\n",
    "            t_hat = t_iter[eq_idx]\n",
    "        x = y - t_hat\n",
    "        # there may be a numerical error such that the constraints are not exactly met.\n",
    "        x[x<0.] = 0.\n",
    "        x[x>1.] = 1.\n",
    "        assert np.abs(x.sum() - 1.) <= 1e-5\n",
    "        assert np.all(x >= 0) and np.all(x <= 1.)\n",
    "        return x\n",
    "\n",
    "\n",
    "ITER = 2000\n",
    "X = np.load('data/X.npy')\n",
    "y = np.load('data/y.npy')\n",
    "model = RobustRegression()\n",
    "train_loss = model.fit(X,y,iter=ITER,lr=0.0001)\n",
    "train_loss_MD = model.fit_MD(X,y,iter=ITER,lr=0.0001)\n",
    "plt.plot(train_loss,'blue')\n",
    "plt.plot(train_loss_MD,'red')\n",
    "plt.title('loss')\n",
    "plt.show()\n",
    "\n",
    "# Matrix Completion\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import os\n",
    "from scipy.special import softmax\n",
    "\n",
    "# find a subgradient of ||A||*\n",
    "def subgradient_nuclear_norm(A,epsilon=1e-5):\n",
    "    m,n = A.shape\n",
    "    U,S,VT = np.linalg.svd(A)\n",
    "    V = VT.T\n",
    "    # rank of A\n",
    "    rank = (S > epsilon).sum()\n",
    "    #print(rank)\n",
    "    U1,U2 = U[:,:rank],U[:,rank:]\n",
    "    V1,V2 = V[:,:rank],V[:,rank:]\n",
    "    # generate a T with singular values uniformly distributed on [0,1]\n",
    "    T_rank = min(m-rank,n-rank)\n",
    "    T = np.random.uniform(0,1,T_rank)\n",
    "    Im = np.identity(m-rank)\n",
    "    In = np.identity(n-rank)\n",
    "    T = Im.dot(np.diag(T)).dot(In)\n",
    "    subgrad = U1.dot(V1.T) + U2.dot(T).dot(V2.T)\n",
    "\n",
    "    return subgrad\n",
    "\n",
    "def project(X,M,O):\n",
    "    A = np.multiply(X,1-O) # for entries outside Ω\n",
    "    B = np.multiply(M,O) # for entries in Ω\n",
    "    return A+B\n",
    "\n",
    "def optimize(X,M,O,step_size,iteration):\n",
    "    loss = 0\n",
    "    for i in range(iteration):\n",
    "        subgrad = subgradient_nuclear_norm(X)\n",
    "        X += - step_size * subgrad\n",
    "        X = project(X,M,O)\n",
    "    loss = ((X-M)**2).sum()/10000\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "k_list = np.array([1,10,20,30,40,50,60,70,80,90,100])**2\n",
    "M = np.array(pd.read_csv('data/MatrixCompletion/M.csv',header=None))\n",
    "O = np.array(pd.read_csv('data/MatrixCompletion/O.csv',header=None))\n",
    "X = np.multiply(M,O)\n",
    "loss1_list = []\n",
    "loss2_list = []\n",
    "for k in k_list:\n",
    "    loss1 = optimize(X,M,O,1/np.sqrt(k),k)\n",
    "    loss2 = optimize(X,M,O,1/k,k)\n",
    "    print(k,loss1,loss2)\n",
    "    loss1_list.append(loss1)\n",
    "    loss2_list.append(loss2)\n",
    "\n",
    "plt.plot(k_list,loss1_list)\n",
    "plt.plot(k_list,loss2_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=red> Writing Problems </font>\n",
    "## 1. Sub-Gradient of Nuclear Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
